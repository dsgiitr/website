<!DOCTYPE html>
<html lang="en-us"><head>
  <!-- Basic Page Needs -->
  <meta charset="utf-8">
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
  
  <meta name="description" content="Machine Learning / Boosting">
  <meta name="author" content="DSG">
  <meta name="generator" content="Hugo 0.104.3" />
  
  <!-- Mobile Specific Metas -->
  <meta name="format-detection" content="telephone=no">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Boosting Decrypted</title>
  <link rel="icon" href="/images/dsg_new_square_logo.png">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.1/css/all.css" integrity="sha384-50oBUHEmvpQ+1lW4y57PTFmhCaXp0ML5d60M1M7uH2+nqUivzIebhndOJK28anvf" crossorigin="anonymous">
  <script src='https://kit.fontawesome.com/a076d05399.js' crossorigin='anonymous'></script>
  <!-- Twitter Bootstrs CSS -->
  <link rel="stylesheet" href="/plugins/bootstrap/bootstrap.min.css">
  <!-- Ionicons Fonts Css -->
  <link rel="stylesheet" href="/plugins/ionicons/ionicons.min.css">
  <!-- animate css -->
  <link rel="stylesheet" href="/plugins/animate-css/animate.css">
  <!-- Hero area slider css-->
  <link rel="stylesheet" href="/plugins/slider/slider.css">
  <!-- slick slider -->
  <link rel="stylesheet" href="/plugins/slick/slick.css">
  <!-- Fancybox -->
  <link rel="stylesheet" href="/plugins/facncybox/jquery.fancybox.css">
  <!-- hover -->
  <link rel="stylesheet" href="/plugins/hover/hover-min.css">
  <!-- template main css file -->
  
  <link rel="stylesheet" href="/css/style.min.css" integrity="" media="screen">
</head><body><section class="top-bar animated-header">
    <div class="container">
        <div class="row">
            <div class="col-lg-12">
                <nav class="navbar navbar-expand-lg navbar-light bg-light">
                    <a class="navbar-brand" href="/">
                        <img id="logo" src="/images/dsg.png" alt="logo">
                    </a>
                    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navigation"
                        aria-controls="navigation" aria-expanded="false" aria-label="Toggle navigation">
                        <span class="navbar-toggler-icon"></span>
                    </button>

                    <div class="collapse navbar-collapse" id="navigation">
                        <ul class="navbar-nav ml-auto">
                            <li class="nav-item">
                                <a class="nav-link"
                                    href="/">Home
                                </a>
                            </li>
                            
                            <li class="nav-item">
                                
                                    <a class="nav-link" href="/#works">Work</a>
                                
                                
                                
                            </li>
                            
                            <li class="nav-item">
                                
                                
                                    <a class="nav-link" href="/events">News</a>
                                
                                
                            </li>
                            
                            <li class="nav-item">
                                
                                
                                    <a class="nav-link" href="/blogs">Blogs</a>
                                
                                
                            </li>
                            
                            <li class="nav-item">
                                
                                
                                    <a class="nav-link" href="/about">About</a>
                                
                                
                            </li>
                            
                    </div>
                </nav>
            </div>
        </div>
    </div>
</section>

<section class="global-page-header">
    <div class="container">
        <div class="row">
            <div class="col-md-12">
                <div class="block">
                    <h2>Boosting Decrypted</h2>
                    <div class="portfolio-meta">
                        <span>Wednesday, Mar 22, 2017</span>|
                        <span> Tags:
                            Boosting
                        </span>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="single-post">
    <div class="container">
        <div class="row">
            <div class="col-md-12">
                
                <div class="post-img">
                    <img class="img-fluid-work" alt="" src="https://miro.medium.com/max/2936/1*jbncjeM4CfpobEnDO0ZTjw.png">
                </div>
                
                <div class="post-content">
                    <p>I know it’s getting arduous to catch up with out daily posts, but it’s just meant to be this. <a href="https://medium.com/data-science-group-iitr/algos-algos-everywhere-f4e684473f14"><strong>12 Algos in 12 days</strong></a>.
Here, we unbox one of the most powerful ML technique used by Grandmasters to win Data Hackathons on <a href="https://www.kaggle.com/">Kaggle</a>. You’ll consider yourself lucky if you understand this properly.</p>
<p>Improving a model accuracy beyond a certain limit can be challenging. This is exactly why you need <a href="https://en.wikipedia.org/wiki/Boosting_(machine_learning)"><em>Boosting</em></a>.</p>
<blockquote>
<p><strong>Don’t give up at half time. Concentrate on winning the second half.</strong></p>
</blockquote>
<hr>
<h1 id="what-is-boosting">What is Boosting?</h1>
<p>A family of machine learning ensemble <em>meta-algorithms</em> in supervised learning that improve the accuracy of ML algorithms.</p>
<blockquote>
<p><strong>Can a set of weak learners create a single strong learner?</strong></p>
</blockquote>
<ul>
<li>
<p><strong>Weak learner</strong>: Any ML algorithm that provides an accuracy slightly better than random guessing.</p>
</li>
<li>
<p><strong>Ensemble</strong>: The overall model built by Boosting is a weighted sum of all of the weak learners. Overall model yields a pretty high accuracy.</p>
</li>
<li>
<p><strong>Meta-algorithm</strong>: It isn’t a machine learning algorithm itself, uses other algorithms to make stronger predictions.</p>
</li>
</ul>
<p><img src="https://cdn-images-1.medium.com/max/1386/1*iKJQDfS4c4DQeKYYWwF5iQ.png" alt="Just take the feel."></p>
<h1 id="types-of-boosting">Types of Boosting</h1>
<h4 id="1-adaboost">1. AdaBoost</h4>
<p><em>First original boosting technique</em>: highly accurate prediction rule by combining many weak and inaccurate rules.
Classic use case: <strong>Face Detection</strong></p>
<blockquote>
<p><strong>Algo explained!</strong></p>
</blockquote>
<p><img src="https://cdn-images-1.medium.com/max/624/1*DoSozt9wfTt4OgiEQM5Ckw.png" alt=""></p>
<p>Above shown is the <strong>sample space</strong> we shall use for classification.</p>
<p><img src="https://cdn-images-1.medium.com/max/374/1*DsQixXamvHj8CQiFCu4sog.png" alt=""></p>
<p><strong>Box1</strong> : Equal weights are assigned to all observations and a decision stump is applied to classify + or - . S1 has generated a vertical line on the left side to classify the data points. This decision stump incorrectly predicted three +. So, we’ll assign more weight to these three data points in our next decision stump.</p>
<p><img src="https://cdn-images-1.medium.com/max/352/1*GNj3-RgWcZXkg4mbdg6GUw.png" alt=""></p>
<p>**Box2 **: The size difference between those three incorrectly predicted and the rest of the data points is clearly visible. Another decision stump (S2) is applied to predict them correctly on the right side of the box. But, this time three - are classified incorrectly. Repeat.</p>
<p><img src="https://cdn-images-1.medium.com/max/398/1*wcVDqOhao3j769pA-SHuvg.png" alt=""></p>
<p>**Box3 : **Here, three - are given higher weights. S3 is applied to predict these misclassified observations correctly. This time, a horizontal line is generated to classify - and +.</p>
<p><img src="https://cdn-images-1.medium.com/max/354/1*rfuvE9saxc5a2OQSzd0hwA.png" alt=""></p>
<p>**Box4 : **Here, we combine S1, S2, and S3 to form a strong prediction having a complex rule as compared to the individual weak learners. Evidently, this algorithm has classified these observations accurately as compared to any of the individual weak learners.</p>
<p>Mostly, we use <strong><em>decision stumps with AdaBoost</em></strong>. But, any machine learning algorithm can be used as base learner if it accepts weights on the training data set. (Both Classification and Regression)</p>
<h3 id="if-you-are-going-through-hell-keep-going-w-churchill">If you are going through hell, keep going. (W. Churchill)</h3>
<blockquote>
<p><strong>Most Important Parameters</strong></p>
</blockquote>
<p>Here, the essential params are explained. Rest can be mugged from <a href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html">Sklearn</a>.</p>
<ul>
<li>
<p><strong>base_estimator</strong>: Base estimator from which boosted ensemble is built. Like in given example we used Decision Tree as base learner.</p>
</li>
<li>
<p><strong>n_estimators</strong>: The maximum number of estimators at which boosting is terminated. In case of <em>perfect fit</em>, the learning procedure is stopped early.</p>
</li>
<li>
<p><strong>learning_rate</strong>: Shrinks the contribution of each tree by learning_rate. There is a <strong><em>trade-off</em></strong> between learning_rate and n_estimators.</p>
</li>
<li>
<p><strong>algorithm</strong>: The <em>SAMME.R algorithm</em> typically converges faster than <em>SAMME</em>, achieving a lower test error with fewer boosting iterations.</p>
</li>
</ul>
<p><img src="https://cdn-images-1.medium.com/max/1616/1*_RkQxs7wZ9nVMQoM4SjL7w.png" alt="Implementation for Noobs. I am also one."></p>
<hr>
<h4 id="2-gradient-boosting">2. Gradient Boosting</h4>
<p>If linear regression was a Toyota Camry, then gradient boosting would be a UH-60 Blackhawk Helicopter. It is the <strong>go-to algorithm</strong> for most of the hackers aiming to win ML competitions. Read carefully!</p>
<p><img src="https://cdn-images-1.medium.com/max/1600/1*t4kMzS-_lvbvGLsx3QEotA.png" alt="See for yourself. Who prefers roads now?"></p>
<ul>
<li>ML technique used for regression and classification problems, it produces a prediction model in the form of an <strong>ensemble of weak prediction models, typically decision trees</strong>.</li>
</ul>
<p>The principle idea behind this algorithm is to construct new base learners which can be maximally correlated with negative gradient of the loss function, associated with the whole ensemble.</p>
<blockquote>
<p><strong>Maths Maths and More Maths:</strong></p>
</blockquote>
<p>Lets consider a regression problem this time. We are given (x1, y1), (x2, y2), … ,(xn, yn), and the task is to fit a model F(x) to minimize the square loss. Suppose we have a model and the model is good but not perfect. There are some mistakes: F(x1) = 0.8, while y1 = 0.9, and F(x2) = 1.4 while y2 = 1.3… How can we improve this model without remove anything from F or change in any parameter of F?</p>
<p>We can add an additional model (regression tree) h to F, so that the new prediction will be F(x) + h(x).</p>
<p>we wish to improve the model such that</p>
<p><img src="https://cdn-images-1.medium.com/max/556/1*GGgWLjyp5i_gIQY-2s_xjw.png" alt=""></p>
<p>Or, equivalently, you wish that</p>
<p><img src="https://cdn-images-1.medium.com/max/568/1*lcm-J-hJzSARm4A8poocMg.png" alt="Yes! It’s that simple."></p>
<p>To get <strong>h,</strong> we fit a model on the points (x1, y1-F(x1)), (x2, y2-F(x2)), …(xn, yn-F(xn)).</p>
<p>The terms (yi-F(xi)) are called residuals. These are the parts where our existing model F cannot do well. The role of h is to compensate the shortcoming of existing model F. If the new model F + h is still not satisfactory, we can add another regression tree.</p>
<blockquote>
<p><strong>How is this related to gradient descent?</strong></p>
</blockquote>
<h4 id="gradient-descent">Gradient Descent</h4>
<p>Minimize a function by moving in the opposite direction of the gradient.</p>
<p><img src="https://cdn-images-1.medium.com/max/NaN/1*q857iI-yTVCilDFBI688TQ.png" alt=""></p>
<p><strong>Loss function : J = (y − F (x))²/2</strong></p>
<p><img src="https://cdn-images-1.medium.com/max/976/1*6W4P7usrtPzOEG_nA0htFQ.png" alt=""></p>
<p><img src="https://cdn-images-1.medium.com/max/442/1*f9LDz5nLtc_edxLFzHacCw.png" alt=""></p>
<p>So consider negative gradient is equivalent to residual in case of Gradient Boosting. So, at each step we are trying to reduce residual and in case of Gradient descent we actually travel in the direction of negative gradient and try to reduce loss.</p>
<p><strong>Python Code</strong></p>
<p><img src="https://cdn-images-1.medium.com/max/1914/1*hRJ8OTDptSQlOL2Bn_hklg.png" alt=""></p>
<ul>
<li>
<p><strong>n_estimators</strong> : int (default=100)</p>
</li>
<li>
<p><strong>learning_rate</strong> : (default=0.1)</p>
</li>
<li>
<p><strong>max_depth</strong> :(default=3) = <em>maximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the tree.</em></p>
</li>
</ul>
<hr>
<h4 id="3-xgboost-extreme-gradient-boosting">3. XGBoost (Extreme Gradient Boosting)</h4>
<p><img src="https://cdn-images-1.medium.com/max/1156/1*JEGYDvCcQvwHUkwkmAZM4w.jpeg" alt=""></p>
<blockquote>
<p>Do one thing before reading this. Take any recent Kaggle competition, and see top 3 winners approaches. If you don’t find use of XGBoost, shoot me!</p>
</blockquote>
<p>Extreme Gradient Boosting (XGBoost) is similar to gradient boosting framework but more efficient. Specifically, XGBoost used a more regularised model formalization to control overfitting, which gives it better performance, which is why it’s also known as ‘<strong>regularized boosting</strong>‘ technique. It has both linear model solver and tree learning algorithms. Moreover, it has the capacity to do parallel computation on a single machine.</p>
<p>This makes XGBoost at least <strong>10 times faster</strong> than existing gradient boosting implementations. It supports various objective functions, including regression, classification and ranking.</p>
<blockquote>
<p><strong><em>Algo Explained:</em></strong></p>
</blockquote>
<p>Main difference between GBM and XGBoost is in their objective function. In case of XGBoost <strong>Objective Function = Training loss + Regularization</strong></p>
<p>I’m going to leave it here, because explaining Mathematics behind XGBoost will take a completely new blog.</p>
<p><img src="https://cdn-images-1.medium.com/max/674/1*Op27K2dChGlfb4_jCsN-7w.png" alt="Objective"></p>
<h3 id="curiosity-is-lust-of-the-mind">Curiosity is lust of the mind</h3>
<p>So, for all the curious mind out there who dwell to explore more! Look <a href="http://xgboost.readthedocs.io/en/latest/model.html#elements-of-supervised-learning">here,</a> <a href="https://arxiv.org/pdf/1603.02754.pdf">here</a> and <a href="https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf">here</a>.</p>
<blockquote>
<p><strong><em>Parameters (Most important for tuning):</em></strong></p>
</blockquote>
<ul>
<li>
<p>**Booster: **You have options like gbtree, gblinear and dart.</p>
</li>
<li>
<p>**learning_rate: **Similar to learning rate in GBM. Try 0.01–0.2</p>
</li>
<li>
<p>**max_depth: **Its same as we used in Random Forest, GBM etc. It controls overfitting. Typically values: 3–10</p>
</li>
</ul>
<h3 id="ohh-these-parameters-keep-repeating-themselves">Ohh! these parameters keep repeating themselves</h3>
<ul>
<li>
<p>**gamma: **Node splitting will be performed only if loss reduction is more than gamma. So, this makes algo conservative.</p>
</li>
<li>
<p>**lambda and alpha: **Lambda is coefficient of L2 regularization and alpha is for L1 regularization. If you have fear of overfitting, tune these well.</p>
</li>
</ul>
<blockquote>
<p><strong><em>Python code:</em></strong></p>
</blockquote>
<p>Those who are wondering that why I described very few parameters above, I said <strong><em>Important.</em></strong></p>
<p><img src="https://cdn-images-1.medium.com/max/1952/1*top0eBWCGv_PSae5hfM68Q.png" alt=""></p>
<p><img src="https://cdn-images-1.medium.com/max/1790/1*aB_tRYOblw2s_cpoZaoATQ.png" alt="And that’s how you predict."></p>
<p>For codes in R, you can refer to <a href="https://www.analyticsvidhya.com/blog/2016/01/xgboost-algorithm-easy-steps/">this article</a> and for tuning the parameters, you can refer to <a href="https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/">this one</a>.</p>
<hr>
<h1 id="is-it-really-that-good">Is it really that good?</h1>
<p>I like this part of the blogpost the most because at the end, this is what matters. Do keep this in mind!</p>
<blockquote>
<p><strong>Pros</strong></p>
</blockquote>
<ul>
<li>
<p>Less error based on ensemble method.</p>
</li>
<li>
<p>Easy to understand.(not for xgboost)</p>
</li>
<li>
<p>Automatically do <strong>feature engineering</strong>.</p>
</li>
<li>
<p><em>Very little</em> data preparation needed for algorithm.</p>
</li>
<li>
<p>Suitable if the initial model is pretty bad</p>
</li>
</ul>
<blockquote>
<p><strong>Cons</strong></p>
</blockquote>
<ul>
<li>
<p>Time and computation expensive.</p>
</li>
<li>
<p>Complexity of the classification increases.</p>
</li>
<li>
<p>Hard to implement in real time platform.</p>
</li>
<li>
<p>Boosting Algorithms generally have 3 parameters which can be fine-tuned, Shrinkage parameter, depth of the tree, the number of trees. Proper training of each of these parameters is needed for a good fit. If parameters are not tuned correctly it may result in over-fitting.</p>
</li>
<li>
<p>In case of <em>imbalanced dataset</em>, decision trees are biased. However, by using proper splitting criteria, this issue can be resolved.</p>
</li>
</ul>
<hr>
<h4 id="references">References</h4>
<ol>
<li>
<p><a href="https://github.com/dmlc/xgboost/blob/master/demo/README.md">Awesome Resources github rep</a>o (IF YOU DON’T OPEN THIS, BLOG WAS NOT WORTH A READ)</p>
</li>
<li>
<p><a href="https://www.analyticsvidhya.com/blog/2015/11/quick-introduction-boosting-algorithms-machine-learning/">AV Blog</a> on Boosting</p>
</li>
<li>
<p><a href="http://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/">Machine Learning Mastery</a></p>
</li>
<li>
<p><a href="https://www.youtube.com/playlist?list=PLvFFyDr-FYmkyik6qssrXFpwbz_LJF_wN">Ensemble Methods</a></p>
</li>
<li>
<p><a href="http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/">Kaggle Master Explains Boosting</a></p>
</li>
<li>
<p><a href="http://xgboost.readthedocs.io/en/latest/model.html">XGBoost docs</a></p>
</li>
<li>
<p><a href="http://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf">XGBoost research paper</a></p>
</li>
<li>
<p><a href="http://ww.web.stanford.edu/~hastie/Papers/SII-2-3-A8-Zhu.pdf">Multi-Class Adaboost</a></p>
</li>
<li>
<p>Sklearn boosting <a href="http://scikit-learn.org/stable/modules/ensemble.html">module</a></p>
</li>
</ol>
<h4 id="footnotes"><strong>Footnotes:</strong></h4>
<p>One suggestion is that do not miss out references, by reading them only you can understand algorithm properly.</p>
<p>Btw, this completes third algo of our <a href="https://medium.com/data-science-group-iitr/algos-algos-everywhere-f4e684473f14"><strong>Algorithm series</strong></a>. This is really exciting for us. Cheers!</p>
<p>Hit ❤ if this makes you little bit more intelligent.</p>
<p>Co-author: <a href="https://medium.com/u/a838bd18af35">Gaurav Jindal</a>
Editors: <a href="https://medium.com/u/ffd0563b6405">Ajay Unagar</a> and <a href="https://medium.com/u/ae175187b1b9">Akhil Gupta</a></p>

                </div>
            </div>
        </div>
    </div>
</section>

<!-- Footer Section Start -->
<footer id="footer">
    <div class="container" id="footer-container">
        <div class="row content-justify-between">
            <div style="width: 100%;">
                <p class="copyright">
                    Contact Us
                </p>
            </div>
            <div class="col-md-7 col-12 text-lg-left text-md-left">
                <!-- Social Media -->
                <ul class="social">
                    
                    <li><a href="mailto:dsg@iitr.ac.in"><i class="ion-android-mail"></i></a></li>
                    
                    <li><a href="https://www.facebook.com/dsgiitr/"><i class="ion-social-facebook"></i></a></li>
                    
                    <li><a href="https://github.com/dsgiitr"><i class="ion-social-github"></i></a></li>
                    
                    <li><a href="https://www.linkedin.com/company/26638705/"><i class="ion-social-linkedin"></i></a></li>
                    
                    <li><a href="https://twitter.com/dsg_iitr"><i class="ion-social-twitter"></i></a></li>
                    
                    <li><a href="https://www.instagram.com/dsgiitr/"><i class="ion-social-instagram"></i></a></li>
                    
                    <li><a href="https://discord.com/invite/ATDN5D9M"><i class="fab fa-discord"></i></a></li>
                    
                    
                    <li style="float: right; color: #FFFFFF;"><i class="ion-closed-captioning"></i> DSG IITR</li>
                    
                </ul>
            </div>
            <div class="col-md-5 col-12 text-lg-left text-md-left location-foot">
                
                <div style="float: right; color: #FFFFFF;">
                    <i class="ion-ios-location" style="color: #000;"></i> New Sac Building, IIT Roorkee
                </div>
                
            </div>
        </div>
    </div>
    
    <script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>
</footer>
<!-- footer section end -->


<!-- jquery -->
<script src="/plugins/jQurey/jquery.min.js"></script>
<!-- Form Validation -->
<script src="/plugins/form-validation/jquery.form.js"></script>
<script src="/plugins/form-validation/jquery.validate.min.js"></script>
<!-- slick slider -->
<script src="/plugins/slick/slick.min.js"></script>
<!-- bootstrap js -->
<script src="/plugins/bootstrap/bootstrap.min.js"></script>
<!-- wow js -->
<script src="/plugins/wow-js/wow.min.js"></script>
<!-- slider js -->
<script src="/plugins/slider/slider.js"></script>
<!-- Fancybox -->
<script src="/plugins/facncybox/jquery.fancybox.js"></script>
<!-- template main js -->

<script src="/js/script.min.js"></script>
<!-- google analitycs -->
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', '', 'auto');
    ga('send', 'pageview');
</script>
<script type="text/javascript" src="/plugins/particlesjs/particles.min.js"></script>
<script type="text/javascript" src="/plugins/particlesjs/demo/js/app.js"></script></body>
</html>