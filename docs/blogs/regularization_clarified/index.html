<!DOCTYPE html>
<html lang="en-us"><head>
  <!-- Basic Page Needs -->
  <meta charset="utf-8">
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
  
  <meta name="description" content="Machine Learning">
  <meta name="author" content="DSG">
  <meta name="generator" content="Hugo 0.100.1" />
  
  <!-- Mobile Specific Metas -->
  <meta name="format-detection" content="telephone=no">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Regularization. Clarified.</title>
  <link rel="icon" href="/images/dsg_new_square_logo.png">

  <!-- Twitter Bootstrs CSS -->
  <link rel="stylesheet" href="/plugins/bootstrap/bootstrap.min.css">
  <!-- Ionicons Fonts Css -->
  <link rel="stylesheet" href="/plugins/ionicons/ionicons.min.css">
  <!-- animate css -->
  <link rel="stylesheet" href="/plugins/animate-css/animate.css">
  <!-- Hero area slider css-->
  <link rel="stylesheet" href="/plugins/slider/slider.css">
  <!-- slick slider -->
  <link rel="stylesheet" href="/plugins/slick/slick.css">
  <!-- Fancybox -->
  <link rel="stylesheet" href="/plugins/facncybox/jquery.fancybox.css">
  <!-- hover -->
  <link rel="stylesheet" href="/plugins/hover/hover-min.css">
  <!-- template main css file -->
  
  <link rel="stylesheet" href="/css/style.min.css" integrity="" media="screen">
</head><body><section class="top-bar animated-header">
    <div class="container">
        <div class="row">
            <div class="col-lg-12">
                <nav class="navbar navbar-expand-lg navbar-light bg-light">
                    <a class="navbar-brand" href="/">
                        <img id="logo" src="/images/dsg.png" alt="logo">
                    </a>
                    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navigation"
                        aria-controls="navigation" aria-expanded="false" aria-label="Toggle navigation">
                        <span class="navbar-toggler-icon"></span>
                    </button>

                    <div class="collapse navbar-collapse" id="navigation">
                        <ul class="navbar-nav ml-auto">
                            <li class="nav-item">
                                <a class="nav-link"
                                    href="/">Home
                                </a>
                            </li>
                            
                            <li class="nav-item">
                                
                                    <a class="nav-link" href="/#works">Work</a>
                                
                                
                                
                            </li>
                            
                            <li class="nav-item">
                                
                                
                                    <a class="nav-link" href="/events">News</a>
                                
                                
                            </li>
                            
                            <li class="nav-item">
                                
                                
                                    <a class="nav-link" href="/blogs">Blogs</a>
                                
                                
                            </li>
                            
                            <li class="nav-item">
                                
                                
                                    <a class="nav-link" href="/about">About</a>
                                
                                
                            </li>
                            
                    </div>
                </nav>
            </div>
        </div>
    </div>
</section>

<section class="global-page-header">
    <div class="container">
        <div class="row">
            <div class="col-md-12">
                <div class="block">
                    <h2>Regularization. Clarified.</h2>
                    <div class="portfolio-meta">
                        <span>Sunday, Apr 2, 2017</span>|
                        <span> Tags:
                            Regualrization, Machine Learning
                        </span>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="single-post">
    <div class="container">
        <div class="row">
            <div class="col-md-12">
                
                <div class="post-img">
                    <img class="img-fluid-work" alt="" src="https://cdn-images-1.medium.com/max/2140/1*6v7Mhm2CU6j89sNJP9I-wA.png">
                </div>
                
                <div class="post-content">
                    <p>The end is near. No, not the world, but the <a href="https://medium.com/data-science-group-iitr/algos-algos-everywhere-f4e684473f14">12A12D</a> series. After <a href="https://medium.com/data-science-group-iitr/linear-regression-back-to-basics-e4819829d78b">Linear Regression</a>, it’s time to add more DS flavour.</p>
<p>This will teach you a new technique used in case of <a href="http://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/"><em>Overfitting</em></a>.</p>
<blockquote>
<p><strong>I believe this is not the end, but the beginning.</strong></p>
</blockquote>
<hr>
<h1 id="problem-of-overfitting">Problem of Overfitting</h1>
<p>Occurs when you build a model that <em>not only</em> captures the signal, but also the <strong>noise</strong> in a dataset.
We want to create models that generalise and perform well on different data-points - <strong><em>AVOID Overfitting!</em></strong>
In comes Regularization, which is a powerful mathematical tool for reducing overfitting within our model. In this article, I have explained the complex science behind ‘<em>Ridge Regression</em>‘ and ‘<em>Lasso Regression</em>‘ which are the most fundamental regularization techniques, sadly still not used by many.</p>
<p><strong>Overfitting</strong> or <strong>High Variance</strong> is caused by a hypothesis function that fits the available data but does not generalise well to predict new data. It is usually caused by a complicated function that creates a lot of unnecessary curves and angles unrelated to the data.</p>
<p><img src="https://cdn-images-1.medium.com/max/800/1*sfcTjeaU55u4mac2km8ZGA.jpeg" alt="As tough as it gets."></p>
<p>This behaviour of the model is not desired as it has very poor predictive power. There are <strong>two</strong> main options to address the issue of overfitting:</p>
<ul>
<li>
<p>Reduce the number of features</p>
</li>
<li>
<p><strong><em>Regularization:</em></strong> When a lot of slightly <strong>useful</strong> features are there.</p>
</li>
</ul>
<hr>
<h1 id="what-is-regularization">What is Regularization?</h1>
<p><img src="https://cdn-images-1.medium.com/max/1444/1*zs6ffbEmnAjR_ctWARuKtw.png" alt="Rings a bell?"></p>
<p><strong>Linear Regression:</strong> The parameters are estimated using the Least Squares approach, where the cost function i.e. the sum of squared residuals (RSS) are minimised.</p>
<p><img src="https://cdn-images-1.medium.com/max/4844/1*gMM8HT1z_Y7MDJFQrSeBnQ.gif" alt=""></p>
<p>To perform <strong>Regularization,</strong> we will be modifying our Cost Function by adding a penalty to RSS. By adding a penalty to the Cost Function, the values of the parameters would decrease and thus the overfitted model gradually starts to <strong>smooth out</strong> depending on the magnitude of the penalty added.</p>
<p><img src="https://cdn-images-1.medium.com/max/898/1*NChkexnmT0KRnjcUF1JvDw.png" alt="After Regularization | Overfitted Model"></p>
<hr>
<h1 id="ridge-regression">Ridge Regression</h1>
<p>It performs ‘<strong>L2 regularization’</strong>, i.e. adds penalty equivalent to <strong>square of the magnitude</strong> of coefficients. Thus, it optimises the following:</p>
<blockquote>
<p><strong>_Objective = RSS + α _ (sum of square of coefficients)*</strong></p>
</blockquote>
<p>Here, <em>α(alpha)</em> is the tuning parameter which balances the amount of emphasis given to minimising RSS vs minimising sum of square of coefficients. It can take various values:</p>
<h4 id="α--0"><strong>α = 0:</strong></h4>
<ul>
<li>
<p>The objective becomes same as simple linear regression.</p>
</li>
<li>
<p>We’ll get the same coefficients as simple linear regression.</p>
</li>
</ul>
<h4 id="α--"><strong>α = ∞:</strong></h4>
<ul>
<li>The coefficients will be <strong><em>zero</em></strong>. Why?
Because of infinite weightage on square of coefficients, anything less than zero will make the objective infinite.</li>
</ul>
<h4 id="0--α--"><strong>0 &lt; α &lt; ∞:</strong></h4>
<ul>
<li>
<p>The magnitude of α will decide the weightage given to different parts of objective.</p>
</li>
<li>
<p>The coefficients will be somewhere between 0 and ones for simple linear regression.</p>
</li>
</ul>
<p>A snippet explaining how to execute Ridge Regression in <strong>Python</strong> is shown below. For further clarification on the syntax, one can visit <a href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html">Sklearn</a>.</p>
<div class="highlight"><div style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">7
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">8
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>from sklearn.linear_model import Ridge
</span></span><span style="display:flex;"><span>import numpy as np
</span></span><span style="display:flex;"><span>n_samples, n_features = 10, 5
</span></span><span style="display:flex;"><span>np.random.seed(0)
</span></span><span style="display:flex;"><span>y = np.random.randn(n_samples)
</span></span><span style="display:flex;"><span>X = np.random.randn(n_samples, n_features)
</span></span><span style="display:flex;"><span>clf = Ridge(alpha=1.0)
</span></span><span style="display:flex;"><span>clf.fit(X, y)
</span></span></code></pre></td></tr></table>
</div>
</div><hr>
<h1 id="lasso-regression">Lasso Regression</h1>
<p>LASSO stands for <strong><em>Least Absolute Shrinkage and Selection Operator</em></strong>. I know it doesn’t give much of an idea but there are 2 key words here - <em>absolute</em> and <em>selection</em>.</p>
<p>Lasso regression performs <strong>L1 regularization</strong>, i.e. it adds a factor of sum of absolute value of coefficients in the optimisation objective.</p>
<blockquote>
<p><strong>Objective = RSS + α * (sum of absolute value of coefficients)</strong></p>
</blockquote>
<p>Here, α (alpha) works similar to that of ridge. Like that of ridge, α can take various values and provide a trade-off between balancing RSS and magnitude of coefficients.</p>
<p>So till now its *appearing to be very similar to Ridge, *but hold on you’ll know the difference by the time we finish. Like before, snippet follows.
For further clarification you can again visit <a href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html">Sklearn</a>.</p>
<div class="highlight"><div style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">7
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>from sklearn import linear_model
</span></span><span style="display:flex;"><span>clf = linear_model.Lasso(alpha=0.1)
</span></span><span style="display:flex;"><span>clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(clf.coef_)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(clf.intercept_)
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="selection-of-α">Selection of α</h4>
<p>Alpha can be adjusted to help you find a good fit for your model.</p>
<ul>
<li>
<p>However, a value that is <strong>too low</strong> might not do anything.</p>
</li>
<li>
<p>One that is <strong>too high</strong> might actually cause you to under-fit the model and lose valuable information.</p>
</li>
</ul>
<p>It’s up to the user to find the sweet spot. Cross validation using different values of alpha can help you to identify the <em>optimal alpha</em> that produces the lowest out of sample error.</p>
<h4 id="key-differences-between-ridge-and-lasso-regression">Key differences between Ridge and Lasso Regression</h4>
<p><strong>Ridge:</strong> It includes all (or none) of the features in the model. Thus, the major advantage of ridge regression is coefficient shrinkage and reducing model complexity.</p>
<p><strong>Lasso:</strong> Along with shrinking coefficients, lasso performs <strong>feature selection</strong> as well. (Remember the ‘<em>selection</em>‘ in the lasso full-form?) As we observed earlier, some of the coefficients become exactly zero, which is equivalent to the particular feature being excluded from the model.</p>
<p>But why is it that the lasso, unlike ridge regression, results in coefficient estimates that are exactly equal to zero? Lets explain it in detail in the next section.</p>
<hr>
<h1 id="variable-selection-property-of-lasso"><strong>Variable Selection Property of Lasso</strong></h1>
<p>Before explaining this property, let’s look at another way of writing minimisation objective. One can show that the lasso and ridge regression coefficient estimates solve the problems respectively.</p>
<p><img src="https://cdn-images-1.medium.com/max/1142/1*T-DWh1s4XG6I_bVH9bj1Dw.png" alt="."></p>
<p>In other words, for every value of α, there is some ‘<strong><em>s’</em></strong> such that the equations (old and new cost functions) will give the same coefficient estimates.
When p=2, then (6.8) indicates that the lasso coefficient estimates have the smallest RSS out of all points that lie within the diamond defined by |β1|+ |β2|≤s.
Similarly, the ridge regression estimates have the smallest RSS out of all points that lie within the circle defined by (β1)²+(β2)²≤s</p>
<p>Now, the above formulations can be used to shed some light on the issue.</p>
<p><img src="https://cdn-images-1.medium.com/max/1600/1*kyW0FzPrnQT7puibrLFhAA.png" alt=""></p>
<p>The <strong>least squares solution</strong> is marked as βˆ, while the blue diamond and circle represent the lasso and ridge regression constraints as explained above.
If <strong><em>‘s’</em></strong> is sufficiently large, then the constraint regions will contain βˆ, and so the ridge regression and lasso estimates will be the same as the least squares estimates. (Such a large value of s corresponds to α=0 in the original cost function). However, in figure, the least squares estimates lie outside of the diamond and the circle, and so the least squares estimates are not the same as the lasso and ridge regression estimates. The ellipses that are centered around βˆ represent regions of constant RSS.</p>
<p>In other words, all of the points on a given ellipse share a common value of the RSS. As the ellipses expand away from the least squares coefficient estimates, the RSS increases. The above equations indicate that the lasso and ridge regression coefficient estimates are given by the first point at which an ellipse contacts the constraint region.</p>
<p>Since, ridge regression has a circular constraint with no sharp points, this intersection will not generally occur on an axis, and so the ridge regression coefficient estimates will be exclusively non-zero.
However, the lasso constraint has corners at each of the axes, and so the ellipse will often intersect the constraint region at an axis. When this occurs, one of the coefficients will equal zero. In higher dimensions, many of the coefficient estimates may equal zero simultaneously. In figure, the intersection occurs at β1=0, and so the resulting model will only include β2.</p>
<p><img src="https://cdn-images-1.medium.com/max/2048/1*9UXnmsC0YqLlbfTkawxeFw.jpeg" alt="Because, this was boring. Tad bit."></p>
<hr>
<h1 id="applications"><strong>Applications</strong></h1>
<ul>
<li>
<p><strong>Ridge:</strong> In majority of the cases, it is used to <em>prevent overfitting</em>. Since it includes all the features, it is not very useful in case of exorbitantly high features, say in millions, as it will pose computational challenges.</p>
</li>
<li>
<p><strong>Lasso:</strong> Since it provides <em>sparse solutions</em>, it is generally the model of choice (or some variant of this concept) for modelling cases where the features are in millions or more. In such a case, getting a sparse solution is of great computational advantage as the features with zero coefficients can simply be ignored.</p>
</li>
</ul>
<blockquote>
<p><strong>Key Notes</strong></p>
</blockquote>
<p>Regularization techniques has its applications everywhere like it has been used in <strong><em>medical sciences</em></strong> to analyse and then study the syphilis data. It was even used for analysis of <strong><em>agricultural economics</em></strong> where the problem of multicollinearity was encountered in past.</p>
<p><strong><em>Note:</em></strong> While performing regularization techniques, you should standardise your input dataset so that it is distributed according to N(0,1), since solutions to the regularised objective function depend on the scale of your features.</p>
<p>A technique known as <strong>Elastic Nets,</strong> which is a combination of Lasso and Ridge regression is used to tackle the limitations of both Ridge and Lasso Regression. One can refer to this <a href="http://stats.stackexchange.com/questions/184029/what-is-elastic-net-regularization-and-how-does-it-solve-the-drawbacks-of-ridge">Link</a> for further details regarding this technique.</p>
<hr>
<h4 id="references">References</h4>
<ol>
<li>
<p>Stanford University <a href="http://statweb.stanford.edu/~tibs/sta305files/Rudyregularization.pdf">slides</a> (For in depth mathematics)</p>
</li>
<li>
<p><a href="http://criticaldensity.blogspot.in/2017/01/predicting-house-prices-with.html">Implementation</a> on <a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques">Kaggle</a> dataset.</p>
</li>
<li>
<p><a href="http://scikit-learn.org/stable/modules/linear_model.html">Scikit Learn</a> documentation.</p>
</li>
</ol>
<h4 id="footnotes">Footnotes</h4>
<p>This was one of the most tech-heavy algorithm in our blog series. I hope that you had a great learning experience. Thanks for supporting us!
Here’s all from 12A12D. Till next time, keep learning and improving.</p>
<p>Thanks for reading. :)
<em>And, ❤ if this was a good read. Enjoy!</em></p>
<p>Editor: <a href="https://medium.com/u/ae175187b1b9">Akhil Gupta</a></p>

                </div>
            </div>
        </div>
    </div>
</section>

<!-- Footer Section Start -->
<footer id="footer">
    <div class="container" id="footer-container">
        <div class="row content-justify-between">
            <div style="width: 100%;">
                <p class="copyright">
                    Contact Us
                </p>
            </div>
            <div class="col-md-7 col-12 text-lg-left text-md-left">
                <!-- Social Media -->
                <ul class="social">
                    
                    <li><a href="mailto:dsg@iitr.ac.in"><i class="ion-android-mail"></i></a></li>
                    
                    <li><a href="https://www.facebook.com/dsgiitr/"><i class="ion-social-facebook"></i></a></li>
                    
                    <li><a href="https://github.com/dsgiitr"><i class="ion-social-github"></i></a></li>
                    
                    <li><a href="https://www.linkedin.com/company/26638705/"><i class="ion-social-linkedin"></i></a></li>
                    
                    <li><a href="https://twitter.com/dsg_iitr"><i class="ion-social-twitter"></i></a></li>
                    
                    
                    <li style="float: right; color: #FFFFFF;"><i class="ion-closed-captioning"></i> DSG IITR</li>
                    
                </ul>
            </div>
            <div class="col-md-5 col-12 text-lg-left text-md-left location-foot">
                
                <div style="float: right; color: #FFFFFF;">
                    <i class="ion-ios-location" style="color: #000;"></i> New Sac Building, IIT Roorkee
                </div>
                
            </div>
        </div>
    </div>
    
    <script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>
</footer>
<!-- footer section end -->


<!-- jquery -->
<script src="/plugins/jQurey/jquery.min.js"></script>
<!-- Form Validation -->
<script src="/plugins/form-validation/jquery.form.js"></script>
<script src="/plugins/form-validation/jquery.validate.min.js"></script>
<!-- slick slider -->
<script src="/plugins/slick/slick.min.js"></script>
<!-- bootstrap js -->
<script src="/plugins/bootstrap/bootstrap.min.js"></script>
<!-- wow js -->
<script src="/plugins/wow-js/wow.min.js"></script>
<!-- slider js -->
<script src="/plugins/slider/slider.js"></script>
<!-- Fancybox -->
<script src="/plugins/facncybox/jquery.fancybox.js"></script>
<!-- template main js -->

<script src="/js/script.min.js"></script>
<!-- google analitycs -->
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', '', 'auto');
    ga('send', 'pageview');
</script>
<script type="text/javascript" src="/plugins/particlesjs/particles.min.js"></script>
<script type="text/javascript" src="/plugins/particlesjs/demo/js/app.js"></script></body>
</html>