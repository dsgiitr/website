<!DOCTYPE html>
<html lang="en-us"><head>
  <!-- Basic Page Needs -->
  <meta charset="utf-8">
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
  
  <meta name="description" content="Machine Learning / Clustering ">
  <meta name="author" content="DSG">
  <meta name="generator" content="Hugo 0.65.3" />
  
  <!-- Mobile Specific Metas -->
  <meta name="format-detection" content="telephone=no">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Clustering Described</title>
  <link rel="icon" href="/images/dsg.ico">

  <!-- Twitter Bootstrs CSS -->
  <link rel="stylesheet" href="/plugins/bootstrap/bootstrap.min.css">
  <!-- Ionicons Fonts Css -->
  <link rel="stylesheet" href="/plugins/ionicons/ionicons.min.css">
  <!-- animate css -->
  <link rel="stylesheet" href="/plugins/animate-css/animate.css">
  <!-- Hero area slider css-->
  <link rel="stylesheet" href="/plugins/slider/slider.css">
  <!-- slick slider -->
  <link rel="stylesheet" href="/plugins/slick/slick.css">
  <!-- Fancybox -->
  <link rel="stylesheet" href="/plugins/facncybox/jquery.fancybox.css">
  <!-- hover -->
  <link rel="stylesheet" href="/plugins/hover/hover-min.css">
  <!-- template main css file -->
  
  <link rel="stylesheet" href="/css/style.min.css" integrity="" media="screen">
</head><body><section class="top-bar animated-header">
    <div class="container">
        <div class="row">
            <div class="col-lg-12">
                <nav class="navbar navbar-expand-lg navbar-light bg-light">
                    <a class="navbar-brand" href="/">
                        <img id="logo" src="/images/dsg.png" alt="logo">
                    </a>
                    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navigation"
                        aria-controls="navigation" aria-expanded="false" aria-label="Toggle navigation">
                        <span class="navbar-toggler-icon"></span>
                    </button>

                    <div class="collapse navbar-collapse" id="navigation">
                        <ul class="navbar-nav ml-auto">
                            <li class="nav-item">
                                <a class="nav-link"
                                    href="/">Home
                                </a>
                            </li>
                            
                            <li class="nav-item">
                                
                                    <a class="nav-link" href="/#works">Work</a>
                                
                                
                                
                            </li>
                            
                            <li class="nav-item">
                                
                                
                                    <a class="nav-link" href="/events">News</a>
                                
                                
                            </li>
                            
                            <li class="nav-item">
                                
                                
                                    <a class="nav-link" href="/blogs">Blogs</a>
                                
                                
                            </li>
                            
                            <li class="nav-item">
                                
                                
                                    <a class="nav-link" href="/about">About</a>
                                
                                
                            </li>
                            
                    </div>
                </nav>
            </div>
        </div>
    </div>
</section>

<section class="global-page-header">
    <div class="container">
        <div class="row">
            <div class="col-md-12">
                <div class="block">
                    <h2>Clustering Described</h2>
                    <div class="portfolio-meta">
                        <span>Monday, Jan 1, 0001</span>|
                        <span> Tags:
                            Clustering Analysis
                        </span>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="single-post">
    <div class="container">
        <div class="row">
            <div class="col-md-12">
                
                <div class="post-img">
                    <img class="img-fluid-work" alt="" src="https://scikit-learn.org/stable/_images/sphx_glr_plot_dbscan_0011.png">
                </div>
                
                <div class="post-content">
                    <p>After Supervised Learning algorithms, it’s time to have a look at the most popular Unsupervised method. Here, we present to you - <strong><em>Clustering</em></strong>, and it’s variants.</p>
<p>Let’s look at it’s simplicity <a href="https://en.wikipedia.org/wiki/Cluster_analysis">here</a></p>
<p>In our daily life, we group different activities according to their utility. This grouping is what you need to learn.</p>
<blockquote>
<p><strong>A winner is just a loser who tried one more time. Keep trying.</strong></p>
</blockquote>
<hr>
<h1 id="what-is-clustering">What is Clustering?</h1>
<ul>
<li>
<p>How does a <em>recommendation system</em> work?</p>
</li>
<li>
<p>How does a company decide the location for their new store so as to generate** **maximum **profit**?</p>
</li>
</ul>
<p>It’s an <a href="http://www.gatsby.ucl.ac.uk/~dayan/papers/dun99b.pdf">unsupervised learning</a> algorithm which groups the given data, such that <strong>data points with similar behaviour</strong> are merged into one group.</p>
<p>Main aim is to segregate the various data points into different groups called <em>clusters</em> such that entities in a particular group comparatively have more similar traits than entities in another group.
*At the end, <strong><em>each data point is assigned to one of the group</em></strong>.</p>
<p>Clustering algorithm <strong>does not</strong> predict an outcome or target variable but can be used to improve predictive model. Predictive models can be built for clusters to <em>improve the accuracy of our prediction</em>.</p>
<p><img src="https://cdn-images-1.medium.com/max/964/1*rs4650azDz6FP5cRVxs2WQ.png" alt="Easy peasy."></p>
<hr>
<h1 id="types-of-clustering">Types of Clustering</h1>
<p>There exist more than 100 clustering algorithms as of today.
Some of the commonly used are <strong>k-Means</strong>, <strong>Hierarchical</strong>, DBSCAN and OPTICS. Two of these have been covered here:</p>
<h4 id="1-hierarchical-clustering">1. Hierarchical Clustering</h4>
<p>It is a type of connectivity model clustering which is based on the fact that data points that are closer to each other are more similar than the data points lying far away in a data space.</p>
<p>As the name speaks for itself, the hierarchical clustering <strong>forms the hierarchy of the clusters</strong> that can be studied by visualising <a href="https://en.wikipedia.org/wiki/Dendrogram"><em>dendogram</em></a>.</p>
<p><img src="https://cdn-images-1.medium.com/max/1600/1*AplNpGVerhVpqmXf1dKd9w.png" alt="Dendogram."></p>
<blockquote>
<p><strong>How to measure closeness of points?</strong></p>
</blockquote>
<ul>
<li>
<p><strong>Euclidean distance</strong>: ||a-b||2 = √(Σ(ai-bi))</p>
</li>
<li>
<p><strong>Squared Euclidean distance</strong>: ||a-b||22 = Σ((ai-bi)²)</p>
</li>
<li>
<p><strong>Manhattan distance</strong>: ||a-b||¹ = Σ|ai-bi|</p>
</li>
<li>
<p><strong>Maximum distance</strong>:||a-b||^inf = maxi|ai-bi|</p>
</li>
<li>
<p><strong>Mahalanobis distance</strong>: √((a-b)T S-1 (-b)) {where, s : covariance matrix}</p>
</li>
</ul>
<blockquote>
<p><strong>How to calculate distance between two clusters?</strong></p>
</blockquote>
<ol>
<li>
<p><strong>Centroid Distance</strong>: Euclidean distance between mean of data points in the two clusters</p>
</li>
<li>
<p><strong>Minimum Distance</strong>: Euclidean distance between two data points in the two clusters that are closest to each other</p>
</li>
<li>
<p><strong>Maximum Distance</strong> : Euclidean distance between two data points in the two clusters that are farthest to each other</p>
</li>
</ol>
<ul>
<li><strong><em>Focus on Centroid Distance right now!</em></strong></li>
</ul>
<h4 id="algorithm-explained">Algorithm Explained</h4>
<ol>
<li>
<p>Let there be <strong><em>N</em></strong> data points. Firstly, these <em>N</em> data points are assigned to <em>N</em> different clusters with one data point in each cluster.</p>
</li>
<li>
<p>Then, two data points with <strong>minimum euclidean distanc</strong>e between them are merged into a single cluster.</p>
</li>
<li>
<p>Then, two clusters with <strong>minimum centroid distance</strong> between them are merged into a single cluster.</p>
</li>
<li>
<p>This <strong><em>process is repeated</em></strong> until we are left with a single cluster, hence forming hierarchy of clusters.</p>
</li>
</ol>
<p><img src="https://cdn-images-1.medium.com/max/1632/1*ZSPU7LV3vXbdRudDTiff6Q.png" alt="How it is done!"></p>
<blockquote>
<p><strong>How many clusters to form?</strong></p>
</blockquote>
<ol>
<li>
<p><strong>Visualising dendogram:</strong> Best choice of no. of clusters is <em>no. of vertical lines that can be cut by a horizontal line</em>, that can transverse maximum distance vertically without intersecting other cluster.
For eg., in the below case, best choice for no. of clusters will be <strong><em>4</em></strong>.</p>
</li>
<li>
<p><strong>Intuition</strong> and prior knowledge of the data set.</p>
</li>
</ol>
<p><img src="https://cdn-images-1.medium.com/max/1536/1*LBOReupihNEsI6Kot3Q6YQ.png" alt="Focus on A and B."></p>
<h4 id="good-cluster-analysis">Good Cluster Analysis</h4>
<ul>
<li>
<p><strong>Data-points within same cluster share similar profile</strong>: Statistically, check the standard deviation for each input variable in each cluster. A perfect separation in case of cluster analysis is rarely achieved. Hence, even <strong><em>one standard deviation distance</em></strong> between two cluster means is considered to be a good separation.</p>
</li>
<li>
<p><strong>Well spread proportion of data-points among clusters</strong>: There are no standards for this requirement. But a minimum of 5% and maximum of 35% of the total population can be assumed as a safe range for each cluster.</p>
</li>
</ul>
<p><img src="https://cdn-images-1.medium.com/max/1720/1*sqE0Rui1GCUZ9hLeWkJjqg.jpeg" alt="Implementation in Python!"></p>
<hr>
<h4 id="k-means-clustering">K-Means Clustering</h4>
<p>One of the simplest and most widely used unsupervised learning algorithm. It involves a simple way to classify the data set into fixed no. of <strong><em>K</em></strong> clusters . The idea is to define <strong><em>K</em></strong> centroids, one for each cluster.</p>
<p>The final clusters depend on the initial configuration of centroids. So, they should be initialized as far from each other as possible.</p>
<ul>
<li>K-Means is <em>iterative</em> in nature and <em>easy</em> to implement.</li>
</ul>
<h4 id="algorithm-explained-1">Algorithm Explained</h4>
<ul>
<li>Let there be <strong><em>N</em></strong> data points. At first, **_K _**centroids are initialised in our data set representing *K *different clusters.</li>
</ul>
<p><img src="https://cdn-images-1.medium.com/max/686/1*kYXvKVSPVnw86RxgAOBSMA.png" alt="Step 1: N = 5, K = 2"></p>
<ul>
<li>Now, each of the <strong><em>N</em></strong> data points are assigned to closest centroid in the data set and merged with that centroid as a single cluster. In this way, every data point is assigned to one of the centroids.</li>
</ul>
<p><img src="https://cdn-images-1.medium.com/max/690/1*Q2AHrnZ7qRsg14NDxalPVg.png" alt="Step 2: Calculating the centroid of the 2 clusters"></p>
<ul>
<li>Then, <strong><em>K</em></strong> cluster centroids are recalculated and again, each of the <strong><em>N</em></strong> data points are assigned to the nearest centroid.</li>
</ul>
<p><img src="https://cdn-images-1.medium.com/max/686/1*BAp7MPVmDZ0UQWv5ZOUbuw.png" alt="Step 3: Assigning all the data points to the nearest cluster centroid"></p>
<ul>
<li>Step 3 is repeated until no further improvement can be made.</li>
</ul>
<p><img src="https://cdn-images-1.medium.com/max/684/1*3t1EXtGfDtbTeLIwm8MoSQ.png" alt="Step 4: Recalculating the cluster centroid. After this step, no more improvement can be made."></p>
<h3 id="in-this-process-a-loop-is-generated-as-a-result-of-this-loop-k-centroids-change-their-location-step-by-step-until-no-more-change-is-possible">In this process, a loop is generated. As a result of this loop, K centroids change their location step by step until no more change is possible.</h3>
<p>This algorithm aims at minimising the <strong>objective function</strong>:</p>
<p><img src="https://cdn-images-1.medium.com/max/600/1*a_RNgesXtp0lkq5LPSWaLg.jpeg" alt=""></p>
<p>It represent the sum of** euclidean distance** of all the data points from the cluster centroid which is minimised.</p>
<p><img src="https://cdn-images-1.medium.com/max/900/1*jrsZ4iEOmhkMciLvbliBJw.png" alt="Implementation in Python!"></p>
<blockquote>
<p><strong>How to initialize K centroids?</strong></p>
</blockquote>
<ol>
<li>
<p><strong>Forgy:</strong> Randomly assigning K centroid points in our data set.</p>
</li>
<li>
<p><strong>Random Partition:</strong> Assigning each data point to a cluster randomly, and then proceeding to evaluation of centroid positions of each cluster.</p>
</li>
<li>
<p>[<strong>KMeans++]</strong>(<a href="https://en.wikipedia.org/wiki/K-means%2B%2B#Improved_initialization_algorithm):">https://en.wikipedia.org/wiki/K-means%2B%2B#Improved_initialization_algorithm):</a> Used for ***small*** data sets.</p>
</li>
<li>
<p>[<strong>Canopy Clustering]</strong>(<a href="https://en.wikipedia.org/wiki/Canopy_clustering_algorithm):">https://en.wikipedia.org/wiki/Canopy_clustering_algorithm):</a> Unsupervised pre-clustering algorithm used as preprocessing step for K-Means or any Hierarchical Clustering. It helps in speeding up clustering operations on ***large data sets***.</p>
</li>
</ol>
<blockquote>
<p><strong>How to calculate centroid of a cluster?</strong></p>
</blockquote>
<p>Simply the mean of all the data points within that cluster.</p>
<blockquote>
<p><strong>How to find value of K for the dataset?</strong></p>
</blockquote>
<p>In K-Means Clustering, value of **_K _**has to be specified beforehand. It can be determine by any of the following methods:</p>
<ul>
<li><strong>Elbow Method</strong>: Clustering is done on a dataset for varying values of and <strong>SSE (Sum of squared errors)</strong> is calculated for each value of <em>K</em>.
Then, a graph between <em>K</em> and SSE is plotted. Plot formed assumes the shape of an arm. There is a point on the graph where SSE does not decreases significantly with increasing <em>K</em>. This is represented by elbow of the arm and is chosen as the value of <em>K</em>. (OPTIMUM)</li>
</ul>
<p><img src="https://cdn-images-1.medium.com/max/1314/1*sqjquWMTaRHBxCyM1XDhFw.png" alt="Code in Python!"></p>
<p><img src="https://cdn-images-1.medium.com/max/1296/1*ZQ_7QFLnLbE3pr4_Meu0Iw.jpeg" alt="K can be 3 or 4."></p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Silhouette_(clustering)"><strong>Silhouette Score</strong></a>: Used to study the <strong><em>separation distance</em></strong> between the resulting clusters. The silhouette plot displays a measure of how close each point in one cluster is to points in the neighbouring clusters. C<a href="http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html">lick here</a> for complete explanation of the method.</li>
</ul>
<p><img src="https://cdn-images-1.medium.com/max/2572/1*VmwXbcBS02prWvKOAn-z_A.png" alt="More the Algos, powerful the Arsenal."></p>
<hr>
<h1 id="k-means-vs-hierarchical">K-Means v/s Hierarchical</h1>
<ol>
<li>
<p>For <strong>big data</strong>, <strong><em>K-Means</em></strong> is better!
Time complexity of K-Means is linear, while that of hierarchical clustering is quadratic.</p>
</li>
<li>
<p>Results are reproducible in <strong><em>Hierarchical</em></strong>, and not in K-Means, as they depend on intialization of centroids.</p>
</li>
<li>
<p>K-Means requires prior and proper knowledge about the data set for specifying <strong><em>K</em></strong>. In <strong><em>Hierarchical</em></strong>, we can choose no. of clusters by interpreting dendogram.</p>
</li>
</ol>
<hr>
<h4 id="references">References</h4>
<ol>
<li>
<p>AV’s <a href="https://www.analyticsvidhya.com/blog/2013/11/getting-clustering-right-part-ii/">Blog</a> on Clustering</p>
</li>
<li>
<p><a href="https://iitrdsg.wordpress.com/2016/06/15/k-means-clustering-explained/">Blog</a> on K-Means by Akhil Gupta</p>
</li>
<li>
<p><a href="https://www.amazon.in/Python-Machine-Learning-Sebastian-Raschka-ebook/dp/B00YSILNL0?_encoding=UTF8&amp;tag=googinhydr18418-21">Python Machine Learning</a> by Sebastian Raschka</p>
</li>
</ol>
<h4 id="footnotes">Footnotes</h4>
<p>You are getting richer day-by-day. 7 down, 5 more to go!
Start applying for internships. You can rock the interviews. Just stick to <a href="https://medium.com/data-science-group-iitr/algos-algos-everywhere-f4e684473f14"><strong>12A12D</strong></a>.</p>
<p>Thanks for reading. :)
<em>And, ❤ if this was a good read. Enjoy!</em></p>
<p>Co-Authors: <a href="https://medium.com/u/1101baa51aff">Nishant Raj</a> and <a href="https://medium.com/u/33c77ec08bba">Pranjal Khandelwal</a></p>

                </div>
            </div>
        </div>
    </div>
</section>

<!-- Footer Section Start -->
<footer id="footer">
    <div class="container" id="footer-container">
        <div class="row content-justify-between">
            <div style="width: 100%;">
                <p class="copyright">
                    Contact Us
                </p>
            </div>
            <div class="col-md-7 col-12 text-lg-left text-md-left">
                <!-- Social Media -->
                <ul class="social">
                    
                    <li><a href="https://www.facebook.com/dsgiitr/"><i class="ion-social-facebook"></i></a></li>
                    
                    <li><a href="https://github.com/dsgiitr"><i class="ion-social-github"></i></a></li>
                    
                    <li><a href="https://www.linkedin.com/company/26638705/"><i class="ion-social-linkedin"></i></a></li>
                    
                    
                    <li style="float: right; color: #FFFFFF;"><i class="ion-closed-captioning"></i> DSG IITR</li>
                    
                </ul>
            </div>
            <div class="col-md-5 col-12 text-lg-left text-md-left location-foot">
                
                <div style="float: right; color: #FFFFFF;">
                    <i class="ion-ios-location" style="color: #000;"></i> New Sac Building, IIT Roorkee
                </div>
                
            </div>
        </div>
    </div>
    
    <script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>
</footer>
<!-- footer section end -->


<!-- jquery -->
<script src="/plugins/jQurey/jquery.min.js"></script>
<!-- Form Validation -->
<script src="/plugins/form-validation/jquery.form.js"></script>
<script src="/plugins/form-validation/jquery.validate.min.js"></script>
<!-- slick slider -->
<script src="/plugins/slick/slick.min.js"></script>
<!-- bootstrap js -->
<script src="/plugins/bootstrap/bootstrap.min.js"></script>
<!-- wow js -->
<script src="/plugins/wow-js/wow.min.js"></script>
<!-- slider js -->
<script src="/plugins/slider/slider.js"></script>
<!-- Fancybox -->
<script src="/plugins/facncybox/jquery.fancybox.js"></script>
<!-- template main js -->

<script src="/js/script.min.js"></script>
<!-- google analitycs -->
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', '', 'auto');
    ga('send', 'pageview');
</script>
<script type="text/javascript" src="/plugins/particlesjs/particles.min.js"></script>
<script type="text/javascript" src="/plugins/particlesjs/demo/js/app.js"></script></body>
</html>